# KubeCloudOrchestrator
Projet dÃ©montrant lâ€™orchestration dâ€™applications multi-conteneurs avec Kubernetes, de la configuration au dÃ©ploiement automatisÃ©. Il analyse lâ€™architecture, les avantages et les limites de Kubernetes face Ã  Docker et aux machines virtuelles.

[TOC]

# **Quâ€™est-ce que Kubernetes ?**

Une **plateforme open-source dâ€™orchestration de conteneurs** dÃ©veloppÃ©e initialement par Google, aujourdâ€™hui maintenue par la **Cloud Native Computing Foundation (CNCF)**.

Il permet de **dÃ©ployer, gÃ©rer, mettre Ã  lâ€™Ã©chelle et maintenir** des applications conteneurisÃ©es de maniÃ¨re automatique.

### **DÃ©finition simple :**

Kubernetes est au conteneur ce quâ€™un systÃ¨me dâ€™exploitation est Ã  un ordinateur : il gÃ¨re les ressources, planifie les tÃ¢ches et garantit la disponibilitÃ©.



# ARCHITECTURE DE KUBERNETES [[2]](#ref2)

![Architecture de kubernetes](./Images/architecture.png)

## Composantes principales

- Master Node (API Server, Controller Manager, Scheduler, etc.)
- Worker Nodes (Kubelet, Kube Proxy, Pod, Container Runtime(ex. Docker))



## Fonctionnement global et communication interne

### **1. Vue dâ€™ensemble : le rÃ´le de Kubernetes**

Un **cluster Kubernetes** est composÃ© de :

- **1 Master Node** (ou plusieurs pour la haute disponibilitÃ©)
- **Plusieurs Worker Nodes**



### **2. Master Node, le cerveau du cluster**

Câ€™est le **centre de contrÃ´le** du cluster. Il gÃ¨re **oÃ¹ et quand** exÃ©cuter les conteneurs, et surveille leur Ã©tat.

| **a. API Server**                                            | **b. Scheduler**                                             |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| - Câ€™est **lâ€™interface centrale** entre les utilisateurs et le cluster. <br />- Tous les composants (kubectl, nodes, etc.) communiquent via cette API REST. <br />- Chaque commande kubectl (ex. kubectl get pods) passe par elle. | - DÃ©cide **sur quel nÅ“ud** chaque Pod doit Ãªtre exÃ©cutÃ©. <br />- Il se base sur :  les ressources disponibles (CPU, RAM), les contraintes de lâ€™application (affinitÃ©s, labels, etc.). |
| **RÃ´le :** point dâ€™entrÃ©e unique pour gÃ©rer le cluster.      | **RÃ´le :** placement intelligent des Pods dans le cluster.   |
|                                                              |                                                              |
| **c. Controller Manager**                                    | **d. etcd**                                                  |
| - Surveille en permanence lâ€™Ã©tat du cluster. <br />- Compare **lâ€™Ã©tat dÃ©sirÃ©** (dÃ©fini par les fichiers YAML) avec **lâ€™Ã©tat actuel**. <br />- Si un Pod crash, il le redÃ©ploie automatiquement. | - Base de donnÃ©es clÃ©-valeur distribuÃ©e (type NoSQL).  <br />- Contient **toute la configuration du cluster** (Ã©tat des Pods, des Services, des Secrets, etc.). |
| **RÃ´le :** maintenir le cluster conforme Ã  la configuration voulue. | **RÃ´le :** stockage central de lâ€™Ã©tat global du cluster.     |
|                                                              |                                                              |
| **e. Authentication & Authorization**                        |                                                              |
| - GÃ¨re les identitÃ©s et permissions (RBAC, comptes de serviceâ€¦).<br />- ContrÃ´le **qui peut faire quoi** dans le cluster. |                                                              |
| **RÃ´le :** sÃ©curitÃ© et contrÃ´le dâ€™accÃ¨s.                     |                                                              |



### **3. Worker Nodes, les muscles du cluster**

Chaque Node (machine) exÃ©cute **les conteneurs rÃ©els**. Le Master leur dit quoi faire.

| **a. Kubelet**                                               | **b. Proxy (kube-proxy)**                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| - Agent qui communique avec le Master. <br />- ReÃ§oit les instructions du Scheduler (exÃ©cuter un Pod). <br />- Surveille la santÃ© des conteneurs sur le node. | GÃ¨re le **trafic rÃ©seau** entrant et sortant des Pods. Met en place le **load balancing** interne et les **rÃ¨gles dâ€™accÃ¨s**. |
| **RÃ´le :** faire exÃ©cuter les Pods et rapporter leur Ã©tat.   | **RÃ´le :** assurer la communication entre Pods et entre lâ€™extÃ©rieur et le cluster. |
|                                                              |                                                              |
| **c. Docker (ou autre runtime comme containerd, CRI-O)**     | **d. Pods**                                                  |
| - ExÃ©cute les **conteneurs** eux-mÃªmes. <br />- Kubernetes nâ€™exÃ©cute pas directement les conteneurs, il dÃ©lÃ¨gue Ã  Docker/containerd. | - UnitÃ© de base dâ€™exÃ©cution dans Kubernetes. <br />- Contient un ou plusieurs conteneurs qui partagent le mÃªme espace rÃ©seau et stockage. |
| **RÃ´le :** moteur dâ€™exÃ©cution des conteneurs.                | **RÃ´le :** encapsule les conteneurs pour leur fournir un environnement cohÃ©rent. |



### **4. Communication et workflow**

1. Lâ€™utilisateur ou DevOps exÃ©cute une commande via **kubectl (CLI)**.
2. Cette commande est envoyÃ©e au **API Server** du Master Node.
3. Le Master enregistre la configuration dans **etcd**.
4. Le **Scheduler** choisit un Node adaptÃ©.
5. Le **Kubelet** du Node reÃ§oit la consigne, et crÃ©e le Pod avec Docker.
6. Le **kube-proxy** sâ€™assure que le trafic rÃ©seau fonctionne correctement.
7. Le **Controller Manager** vÃ©rifie que le Pod tourne bien.



### **5. Internet et Services**

Le schÃ©ma montre aussi une connexion avec Internet. Les Pods ne sont **pas directement exposÃ©s au monde extÃ©rieur**. Kubernetes utilise des **Services** (LoadBalancer, NodePort, Ingress) pour exposer les applications.

### **En rÃ©sumÃ© :**

| **Ã‰lÃ©ment**                      | **RÃ´le**                                                     |
| -------------------------------- | ------------------------------------------------------------ |
| **API Server**                   | Point central de communication                               |
| **Scheduler**                    | DÃ©cide oÃ¹ placer les pods selon ressources et politiques.    |
| **Controller Manager**           | Surveille et maintient lâ€™Ã©tat dÃ©sirÃ©, orchestre controllers (DeploymentController, ReplicaSet, ...) |
| **etcd**                         | Base de donnÃ©es clÃ©/valeur distribuÃ©e (source de vÃ©ritÃ© du cluster) |
| **Kubelet**                      | Agent sur chaque node qui exÃ©cute pods et rapporte l'Ã©tat.   |
| **kube-proxy**                   | Assure la connectivitÃ© rÃ©seau et load-balancing au niveau Node. |
| **Docker/containerd**            | ExÃ©cute les conteneurs                                       |
| **Pod**                          | UnitÃ© de base dâ€™exÃ©cution                                    |
| **kubectl**                      | Interface CLI pour interagir avec lâ€™API                      |
| **Service (NodePort/ClusterIP)** | Abstraction rÃ©seau pour accÃ©der aux pods.                    |



### **Exemple : dÃ©ploiement dâ€™une app Flask avec** **kubectl apply**

#### **1. Lancer une commande :** 

```bash
kubectl apply -f flask-deployment.yaml
```



#### **2. Communication entre les composants :**

Voici ce qui se passe Ã©tape par Ã©tape :

### **Flux complet:**

![Flux complet de communication](./Images/flow.png)

EXPLICATION:
---------------------

**kubectl (CLI)  -->  API Server**
**(1)** kubectl envoie une requÃªte HTTP REST au Kubernetes API Server
    (ex: https://<master-ip>:6443) pour crÃ©er/modifier une ressource.

**API Server -> AuthN/AuthZ**
**(2)** L'API Server vÃ©rifie l'identitÃ© et permissions (Authentication & Authorization)
    (s'assure que l'utilisateur a le droit de crÃ©er le Deployment).

**API Server -> etcd (Distributed Storage)**
**(3)** Ressource validÃ©e : l'Ã©tat dÃ©sirÃ© est persistÃ© dans etcd
    (ex: "je veux 2 pods flask"; etcd stocke l'Ã©tat dÃ©sirÃ© du cluster).

**Scheduler (notification)**
**(4)** Le Scheduler remarque la nouvelle ressource/pod Ã  placer
    (choisit un Worker Node selon ressources, affinitÃ©s, taints/tolerations).

**Controller Manager**
**(5)** Controller Manager compare Ã©tat dÃ©sirÃ© vs Ã©tat actuel
    (si mismatch, il ordonne au Kubelet du node choisi de crÃ©er le pod).

**Sur le Worker Node**
**(6) et (7)** Kubelet reÃ§oit l'instruction et demande au Container Runtime (Docker/CRI) :

tirer l'image (flask-hello:1.0)
dÃ©marrer le conteneur (crÃ©er le Pod)
    (6b) Le Pod dÃ©marre et effectue son probe d'Ã©tat (Readiness/Liveness).
    (6c) Kubelet signale l'Ã©tat au Master (via API Server).

**Kube-Proxy et rÃ©seau**
**(8)** Kube-Proxy configure les rÃ¨gles rÃ©seau (iptables/ipvs) et le service
    (permet la dÃ©couverte et la communication : autre pod <-> service NodePort:31181).



### **Ce quâ€™il faut retenir :**

- Lâ€™**API Server** est le **point central** : tout passe par lui.
- **etcd** ne parle **directement Ã  personne** sauf Ã  lâ€™API Server.
- Le **Scheduler** et le **Controller Manager** observent lâ€™Ã©tat dans **etcd** via lâ€™API Server (mÃ©canisme *watch*).
- Le **Kubelet** ne crÃ©e rien seul : il agit **uniquement sur ordre du Master**.
  

### ModÃ¨le de virtualisation et isolation des conteneurs

------



## **Ã‰tude de cas : DÃ©ploiement dâ€™une application Flask avec Kubernetes** [[1]](#ref1)

### Description de lâ€™application Flask

C'est une application trÃ¨s simple qui affiche **"Hello World from Kubernetes!"** Ã  l'Ã©cran.



### Environnement utilisÃ© 

- **Minikube** : pour dÃ©ployer un **cluster k8s local** pour les tests,
- **Docker** : utilisÃ© comme **moteur de conteneurisation** (*container runtime*) pour exÃ©cuter les Pods.
- **kubectl** : **interface en ligne de commande (CLI)** servant Ã  interagir avec le **k8s API Server** (crÃ©ation, inspection, gestion des ressources).



### Structure du projet 

Obtenue via la commande suivante

```bash
make tree
```

```basic
.
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ conteneurs_systÃ¨me_de_k8s_.md
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Images
â”‚Â Â  â””â”€â”€ architecture.png
â”œâ”€â”€ install_kubernetes_env.sh
â”œâ”€â”€ k8s
â”‚Â Â  â”œâ”€â”€ base
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ configmap.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kustomization.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ secret.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ overlays
â”‚Â Â      â”œâ”€â”€ dev
â”‚Â Â      â”‚Â Â  â””â”€â”€ kustomization.yaml
â”‚Â Â      â””â”€â”€ prod
â”‚Â Â          â””â”€â”€ kustomization.yaml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_system.sh
```



### Ã‰tapes de dÃ©ploiement automatisÃ©

Il y a le script [install_kubernetes_env.sh](./install_kubernetes_env.sh) qui automatise l'installation de Docker, kubectl et Minikube s'ils ne sont pas encore installÃ©s.

**Pour l'utiliser :**

```bash
make install-k8s_env
```

**Note** : Le script ne rÃ©installera jamais un composant dÃ©jÃ  prÃ©sent. Il affichera simplement sa version et passera au suivant.



**DÃ©ployer sur Kubernetes** dans l'un des deux modes possibles : 

1. **DÃ©veloppement** : lancer automatiquement le script [run_system.sh](./run_system.sh) en mode dev

   ```bash
   make auto-deploy-dev
   ```

2. **Production **: lancer automatiquement le script [run_system.sh](./run_system.sh) en mode prod

   ```bash
   make auto-deploy-prod
   ```

------

#### Sortie attendue pour make auto-deploy-prod :

```basic
amolitho@amolitho:~/InsideKubernetes$ make auto-deploy-prod 
chmod +x run_system.sh
./run_system.sh --prod
==========================================
DÃ©ploiement en environnement: PROD
==========================================

[1/6] VÃ©rification de Minikube...
DÃ©marrage de Minikube...
ğŸ˜„  minikube v1.37.0 sur Ubuntu 24.04
âœ¨  Utilisation du pilote virtualbox basÃ© sur le profil existant
ğŸ‘  DÃ©marrage du nÅ“ud "minikube" primary control-plane dans le cluster "minikube"
ğŸ”„  RedÃ©marrage du virtualbox VM existant pour "minikube" ...
ğŸ³  PrÃ©paration de Kubernetes v1.34.0 sur Docker 28.4.0...
ğŸ”—  Configuration de bridge CNI (Container Networking Interface)...
ğŸ”  VÃ©rification des composants Kubernetes...
    â–ª Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Modules activÃ©s: default-storageclass, storage-provisioner

â—  /usr/bin/kubectl est la version 1.30.14, qui peut comporter des incompatibilitÃ©s avec Kubernetes 1.34.0.
    â–ª Vous voulez kubectl v1.34.0 ? Essayez 'minikube kubectl -- get pods -A'
ğŸ„  TerminÃ© ! kubectl est maintenant configurÃ© pour utiliser "minikube" cluster et espace de noms "default" par dÃ©faut.
âœ“ Minikube dÃ©marrÃ©

[2/6] Configuration de Docker pour Minikube...
âœ“ Docker pointe sur: minikube

[3/6] Build de l'image Docker...
âœ“ Image flask-hello:1.0 existe dÃ©jÃ , skip du build
âœ“ Image flask-hello:1.0 disponible

[4/6] Nettoyage des anciennes ressources...
Aucune ressource Ã  supprimer

[5/6] DÃ©ploiement Kubernetes (prod)...
configmap/flask-config created
secret/flask-secret created
service/flask-service created
deployment.apps/flask-deployment created
Attente du dÃ©marrage des pods...
pod/flask-deployment-6dbf944f88-58xwl condition met
pod/flask-deployment-6dbf944f88-clslf condition met
pod/flask-deployment-6dbf944f88-f4sfs condition met
âš  Timeout ou pods pas encore prÃªts, vÃ©rifiez avec 'kubectl get pods'

[6/6] Ã‰tat du dÃ©ploiement:
==========================
NAME                                READY   STATUS    RESTARTS   AGE
flask-deployment-6dbf944f88-58xwl   1/1     Running   0          60s
flask-deployment-6dbf944f88-clslf   1/1     Running   0          60s
flask-deployment-6dbf944f88-f4sfs   1/1     Running   0          60s

NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
flask-service   NodePort    10.102.5.179   <none>        5600:31181/TCP   61s

==========================================
âœ“ Application dÃ©ployÃ©e avec succÃ¨s!
==========================================

URL d'accÃ¨s:
http://192.168.59.101:31181

Commandes utiles:
  minikube service flask-service      # Ouvrir dans le navigateur
  kubectl logs -l app=flask-app       # Voir les logs
  kubectl get all                     # Voir toutes les ressources
  make delete-prod                  # Nettoyer
==========================================
```



**Voir toutes les ressources : *kubectl get all***

**Sortie attendue:** 

```basic
amolitho@amolitho:~/InsideKubernetes$ kubectl get all
NAME                                    READY   STATUS    RESTARTS   AGE
pod/flask-deployment-6dbf944f88-58xwl   1/1     Running   0          5m2s
pod/flask-deployment-6dbf944f88-clslf   1/1     Running   0          5m2s
pod/flask-deployment-6dbf944f88-f4sfs   1/1     Running   0          5m2s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/flask-service   NodePort    10.102.5.179   <none>        5600:31181/TCP   5m3s
service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP          5d1h

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/flask-deployment   3/3     3            3           5m3s

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/flask-deployment-6dbf944f88   3         3         3       5m3s
```

**Tenter de supprimer tout d'un coup**
```bash
kubectl delete all -l app=flask-app
```

**Sortie attendue:** 

```basic
amolitho@amolitho:~/InsideKubernetes$ kubectl delete all -l app=flask-app
pod "flask-deployment-6dbf944f88-58xwl" deleted
pod "flask-deployment-6dbf944f88-clslf" deleted
pod "flask-deployment-6dbf944f88-f4sfs" deleted
replicaset.apps "flask-deployment-6dbf944f88" deleted
```
**VÃ©rifie toutes les ressources Ã  nouveau : *kubectl get all***

**Sortie attendue:** 

```basic
amolitho@amolitho:~/InsideKubernetes$ kubectl get all
NAME                                    READY   STATUS    RESTARTS   AGE
pod/flask-deployment-6dbf944f88-4zsfs   1/1     Running   0          3m4s
pod/flask-deployment-6dbf944f88-72spq   1/1     Running   0          3m4s
pod/flask-deployment-6dbf944f88-scpwn   1/1     Running   0          3m4s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/flask-service   NodePort    10.102.5.179   <none>        5600:31181/TCP   13m
service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP          5d1h

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/flask-deployment   3/3     3            3           13m

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/flask-deployment-6dbf944f88   3         3         3       3m4s
```
Le rÃ©sultat montre quelque chose dâ€™important : Le Deployment flask-deployment nâ€™a pas Ã©tÃ© supprimÃ©, donc Kubernetes a automatiquement recrÃ©Ã© trois nouveaux pods pour le remplacer.

En clair : On a supprimÃ© les pods et le replica set, mais pas le Deployment, du coup, Kubernetes a dÃ©tectÃ© quâ€™il â€œmanquaitâ€ des pods et les a recrÃ©Ã©s selon la dÃ©finition du dÃ©ploiement.

Câ€™est le comportement normal et voulu dâ€™un Deployment : il garantit quâ€™un nombre fixe de pods tourne en permanence.

Si on veut tout supprimer rÃ©ellement, exÃ©cute :
```bash
kubectl delete deployment flask-deployment
kubectl delete service flask-service
```

Ensuite vÃ©rifie :
```bash
kubectl get all
```
On ne verra alors plus ni pods, ni deployment, ni service liÃ©s Ã  ton app Flask.
```basic
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5d1h
```
On nâ€™a plus que le service â€œkubernetesâ€, qui est gÃ©nÃ©rÃ© automatiquement par le systÃ¨me pour permettre la communication interne entre les composants du cluster, câ€™est normal et on ne dois pas le supprimer.

Tout le reste (pods, services, dÃ©ploiements Flask) a bien Ã©tÃ© supprimÃ©.

L'environnement est donc prÃªt Ã  :

Ãªtre redÃ©ployÃ© proprement (make auto-deploy-prod ou make auto-deploy-dev selon le besoin), ou Ãªtre arrÃªtÃ© proprement via :
```bash
minikube stop
```



# **Analyse critique de Kubernetes**

## **Pourquoi utiliser k8s ?**

Sans Kubernetes, les dÃ©veloppeurs dÃ©ploient et gÃ¨rent les conteneurs (Docker, par exemple) manuellement.
Mais dÃ¨s quâ€™on parle de **plusieurs serveurs, plusieurs conteneurs**, les problÃ¨mes apparaissent :

- Comment redÃ©marrer un conteneur qui crash ?
- Comment distribuer la charge sur plusieurs machines ?
- Comment mettre Ã  jour sans interruption ?
- Comment gÃ©rer des centaines de conteneurs ?

Kubernetes **automatise** tout Ã§a.



### **Ses grands avantages :**

1. **Haute disponibilitÃ©** â€“ redÃ©marre automatiquement les conteneurs en cas de panne.
2. **ScalabilitÃ©** â€“ augmente ou diminue automatiquement le nombre dâ€™instances.
3. **Load balancing** â€“ distribue le trafic entre les pods.
4. **Mises Ã  jour continues** (rolling updates).
5. **Gestion simplifiÃ©e du dÃ©ploiement** sur des environnements hybrides (on-premise ou cloud).

## **k8s apporte des solutions aux problÃ¨mes suivants**

| **ProblÃ¨me (avant K8s)**         | **Solution avec Kubernetes**                     |
| -------------------------------- | ------------------------------------------------ |
| Serveurs difficiles Ã  configurer | DÃ©finis l'infra en YAML (Infrastructure-as-Code) |
| DÃ©ploiements manuels             | Automatisation via kubectl ou CI/CD              |
| Downtime lors des updates        | Rolling updates + rollbacks                      |
| Scaling manuel                   | Horizontal Pod Autoscaler                        |
| Monitoring complexe              | IntÃ©gration Prometheus / Grafana                 |
| Load balancing artisanal         | Services intÃ©grÃ©s                                |
| Pannes imprÃ©visibles             | Self-healing automatique                         |



------

## **Limites et inconvÃ©nients de Kubernetes**

### **1. DÃ©veloppement**

- **Courbe dâ€™apprentissage Ã©levÃ©e** : la comprÃ©hension des composants (API Server, etcd, Scheduler, Kubelet, etc.) demande du temps et une solide base en architecture systÃ¨me.
- **ComplexitÃ© de configuration** : beaucoup dâ€™abstractions (Pods, Services, Deployments, etc.) rendent difficile la configuration initiale et la dÃ©tection dâ€™erreurs.
- **Environnement lourd** : un cluster local (Minikube) consomme plusieurs Go de mÃ©moire et des ressources CPU importantes, parfois difficile Ã  faire tourner sur une machine personnelle.

------

### **2. DÃ©ploiement**

- **Configuration YAML volumineuse** : la gestion des fichiers YAML devient vite fastidieuse pour de gros projets.
- **DÃ©pendances externes** : nÃ©cessite souvent des outils supplÃ©mentaires (Helm, Istio, ArgoCD, etc.) pour un dÃ©ploiement fluide.
- **Debugging difficile** : il nâ€™est pas Ã©vident de savoir quel composant traite une requÃªte Ã  un instant T. Il faut souvent passer par des logs, `kubectl describe`, `kubectl logs`, ou des outils dâ€™observabilitÃ© comme Prometheus/Grafana.

------

### **3. En production**

- **ScalabilitÃ© et performance** : Kubernetes peut gÃ©rer une charge Ã©norme, **mais pas seul**.
  - Pour des millions de paquets/seconde, il faut des **nÅ“uds puissants**, une **infrastructure rÃ©seau optimisÃ©e**, et parfois **des solutions de load balancing externes** (NGINX Ingress, Envoy, MetalLB, etc.).
  - Le goulot dâ€™Ã©tranglement peut venir du **plan de contrÃ´le (API Server + etcd)** sâ€™il nâ€™est pas dimensionnÃ© correctement.
- **ComplexitÃ© du dÃ©bogage** : identifier une panne (Pod crash, rÃ©seau, volume, etc.) nÃ©cessite de croiser les logs de plusieurs composants.
- **Maintenance continue** : mises Ã  jour frÃ©quentes, gestion des versions, surveillance constante â€” Kubernetes **ne supprime pas la complexitÃ©**, il la **dÃ©place**.

------



## **Bonnes pratiques dâ€™utilisation de Kubernetes**

### **1. DÃ©veloppement**

- **Organisation modulaire** : crÃ©er une arborescence claire, par exemple :

  ```bash
  â”œâ”€â”€ k8s                  # RÃ©pertoire principal contenant les configurations Kubernetes
  â”‚   â”œâ”€â”€ base             # Configuration de base commune Ã  tous les environnements
  â”‚   â”‚   â”œâ”€â”€ configmap.yaml        # DÃ©finit les variables de configuration non sensibles
  â”‚   â”‚   â”œâ”€â”€ deployment.yaml    # DÃ©crit le dÃ©ploiement (pods, conteneurs, rÃ©plicas, etc.)
  â”‚   â”‚   â”œâ”€â”€ kustomization.yaml  # Fichier principal pour assembler les ressources de base
  â”‚   â”‚   â”œâ”€â”€ secret.yaml  # Contient les informations sensibles (mots de passe, clÃ©s APIâ€¦)
  â”‚   â”‚   â””â”€â”€ service.yaml # DÃ©finit lâ€™accÃ¨s rÃ©seau (exposition des pods via un service)
  â”‚   â””â”€â”€ overlays          # Configurations spÃ©cifiques Ã  chaque environnement
  â”‚       â”œâ”€â”€ dev           # Environnement de dÃ©veloppement
  â”‚       â”‚   â””â”€â”€ kustomization.yaml # Personnalisation de la base pour le dev (rÃ©plicas, image tagâ€¦)
  â”‚       â””â”€â”€ prod                 # Environnement de production
  â”‚           â””â”€â”€ kustomization.yaml # Personnalisation de la base pour la prod (scaling, ressourcesâ€¦)
  
  ```

  â†’ Facilite la maintenance, les tests et le versionnement.

- **Automatisation** : utiliser un *Makefile* ou des scripts (`make deploy`, `make clean`, etc.) pour Ã©viter les erreurs manuelles.

- **Gestion des ressources** : libÃ©rer les volumes, Pods ou images inutilisÃ©s (`kubectl delete`, `docker system prune`) pour Ã©conomiser de lâ€™espace.

- **Dimensionnement intelligent** : ajuster le nombre de *replicas* selon les besoins, avec une petite marge pour la rÃ©silience.

- **Documentation continue** : noter chaque configuration ou commande clÃ© (README, Wiki interne).

------

### **2. DÃ©ploiement**

- **VÃ©rification de la chaÃ®ne dâ€™exÃ©cution** : sâ€™assurer que Kubernetes utilise bien le runtime configurÃ© (Docker, containerd, etc.) avant dÃ©ploiement.

- **Validation continue** : appliquer `kubectl apply --dry-run=client -f â€¦` pour tester les manifestes sans les exÃ©cuter rÃ©ellement.

- **Observation active** : surveiller les Ã©vÃ©nements et logs :

  ```bash
  kubectl get events --sort-by=.metadata.creationTimestamp
  kubectl logs <pod-name>
  ```

- **Rollback rapide** : toujours prÃ©voir une stratÃ©gie de retour arriÃ¨re (ex. `kubectl rollout undo deployment/<name>`).

------

### **3. En production**

- **Surveillance proactive** : utiliser des outils comme *Prometheus*, *Grafana* et *Alertmanager* pour dÃ©tecter les anomalies tÃ´t.
- **SÃ©curitÃ© stricte** : limiter les accÃ¨s (`RBAC`), chiffrer les secrets, et sÃ©parer les namespaces selon les Ã©quipes ou les projets.
- **ScalabilitÃ© maÃ®trisÃ©e** : configurer lâ€™**Horizontal Pod Autoscaler (HPA)** et le **Cluster Autoscaler** pour absorber les pics de charge.
- **Sauvegarde de lâ€™Ã©tat** : exporter rÃ©guliÃ¨rement les donnÃ©es critiques de `etcd` et des volumes persistants.
- **Mise Ã  jour progressive** : dÃ©ployer en *rolling updates* ou *canary releases* pour minimiser les interruptions.

------



# **Comparaison technique**

| **CatÃ©gorie / CritÃ¨re**          | **Kubernetes (K8s)**                                         | **Docker / Docker Compose**                                  | **Machines Virtuelles (VMs)**                                |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Nature et rÃ´le**               | Orchestrateur de conteneurs Ã  grande Ã©chelle (multi-nÅ“uds).  | Gestionnaire simple de conteneurs multi-services sur une seule machine. | Environnements virtuels complets exÃ©cutÃ©s via un hyperviseur. |
| **Base commune / SimilaritÃ©s**   | - Repose sur le mÃªme noyau Linux (kernel).<br />- Les conteneurs partagent les ressources de la machine hÃ´te.<br />- Chaque Pod est isolÃ© (un Pod affectÃ© nâ€™impacte pas les autres).<br />- Routage automatique entre entitÃ©s (Pods).<br />- Attribution dâ€™adresses IP dynamiques (DHCP interne).<br />- Exposition / masquage de services configurable.<br />- Permet un dÃ©ploiement multi-service cohÃ©rent et reproductible. | - Repose sur le mÃªme kernel.<br />- Isolation entre conteneurs.<br />- Routage interne simple et configurable.<br />- Attribution IP automatique via bridge rÃ©seau.<br />- Excellent pour la cohÃ©rence entre environnements dev/test. | - Fonctionne comme environnement isolÃ© complet.<br />- Supporte rÃ©seau virtuel (routage, DHCP interne).<br />- Permet communication entre VMs via bridge ou NAT.<br />- SÃ©paration stricte entre systÃ¨mes invitÃ©s. |
| **Architecture**                 | DistribuÃ©e (Master Node et Worker Nodes ).<br />ConÃ§u pour orchestrer des clusters complexes. | CentralisÃ©e : Docker Engine + Docker Compose YAML.<br />SimplicitÃ©, tout est local. | - DÃ©pend de lâ€™hyperviseur (VirtualBox, VMwareâ€¦).<br />- Chaque VM possÃ¨de son propre OS invitÃ©. |
| **ComplexitÃ© dâ€™utilisation**     | Ã‰levÃ©e : nÃ©cessite la comprÃ©hension des composants internes (API, Pods, Services, etc.). | Faible Ã  moyenne, rapide Ã  apprendre, idÃ©al pour dev local.  | Moyenne, dÃ©pend des outils de virtualisation utilisÃ©s.       |
| **Consommation de ressources**   | Ã‰levÃ©e, plusieurs Go requis mÃªme en local (Minikube).        | Faible, lÃ©ger, dÃ©marre en quelques secondes.                 | TrÃ¨s Ã©levÃ©e, chaque VM rÃ©serve une quantitÃ© fixe de RAM et CPU. |
| **Partage de ressources**        | Partage le mÃªme kernel hÃ´te (Linux).                         | Idem, partage kernel et ressources.                          | Ne partage pas le kernel hÃ´te : chaque VM a son propre OS => plus lourd. |
| **Automatisation et rÃ©silience** | Auto-scaling, auto-healing, rolling updates, rollback intÃ©grÃ©s. | Manuel (redÃ©marrage et scaling manuels).                     | Snapshots ou scripts pour gestion, souvent manuelle.         |
| **Stockage**                     | Volumes persistants (PV/PVC) gÃ©rÃ©s par K8s.                  | Volumes simples montÃ©s localement.                           | Disques virtuels indÃ©pendants (VDI, VMDK, etc.).             |
| **RÃ©seautage**                   | CoreDNS, Services, Ingress, routage interne, DNS auto.       | RÃ©seau bridge ou host, configuration simple.                 | - RÃ©seau virtuel via hyperviseur (bridge, NAT). <br />- DNS interne non natif sans service externe (pfSenseâ€¦). |
| **DÃ©ploiement / Configuration**  | DÃ©claratif (fichiers YAML + kubectl apply).                  | `docker-compose up` depuis un seul fichier YAML.             | Manuel ou via outils (scripts, Terraform, hyperviseur).      |
| **ScalabilitÃ©**                  | - Horizontale (ajout de Pods) et verticale (plus de ressources). <br />- AutomatisÃ©e (HPA, Cluster Autoscaler). | - LimitÃ©e au matÃ©riel local.<br />- Pas dâ€™auto-scaling sans outils externes. | Lourde,nÃ©cessite crÃ©ation de nouvelles VMs et redÃ©ploiement complet. |
| **Maintenance**                  | - CentralisÃ©e via kubectl et YAML.<br />- Rolling updates, rollback, auto-restart. | Manuelle via redÃ©marrage / rebuild.                          | Snapshots ou restauration dâ€™image complÃ¨te.                  |
| **Interaction Ã©vÃ©nementielle**   | BasÃ©e sur un modÃ¨le dÃ©claratif et rÃ©actif : <br />- API Server publie les changements. <br />- Scheduler, Controller Manager, Kubelet rÃ©agissent aux Ã©vÃ©nements.<br />â†’ **SystÃ¨me auto-rÃ©gulÃ©** : chaque Ã©vÃ©nement rÃ©tablit lâ€™Ã©tat dÃ©sirÃ©. | Ã‰vÃ©nements limitÃ©s au moteur Docker (logs, restart policies). | Peu dâ€™automatisation Ã©vÃ©nementielle : nÃ©cessite scripts ou monitoring externe. |
| **Performance / RapiditÃ©**       | TrÃ¨s performante Ã  grande Ã©chelle mais nÃ©cessite une bonne config. | Ultra rapide et lÃ©ger pour le dÃ©veloppement local.           | Plus lente (OS complet par VM).                              |
| **SÃ©curitÃ©**                     | Fine et modulaire (RBAC, NetworkPolicy, Namespaces).         | Moins granulaire.                                            | TrÃ¨s forte isolation via hyperviseur.                        |
| **Apprentissage**                | Difficile, demande une comprÃ©hension des concepts dâ€™orchestration. | Facile, intuitif et rapide Ã  maÃ®triser.                      | Moyen, dÃ©pend du logiciel de virtualisation.                 |

------



# Quand utiliser quoi ?

| **Technologie**               | **Quand lâ€™utiliser**                                         | **Pourquoi câ€™est le bon choix**                              |
| ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Docker / Docker Compose**   | - En **phase de dÃ©veloppement** ou de **test local**.<br />- Pour **des projets lÃ©gers**, des microservices simples ou des prototypes. | - Facile Ã  configurer et Ã  dÃ©ployer.- DÃ©marrage rapide, consommation faible.<br />- IdÃ©al pour tester des services avant orchestration. |
| **Kubernetes (K8s)**          | - En **production**, pour **dÃ©ploiements Ã  grande Ã©chelle** ou **environnements distribuÃ©s**.<br />- Quand il faut de la **rÃ©silience, du scaling automatique** et du **monitoring centralisÃ©**. | - Orchestration avancÃ©e, tolÃ©rance aux pannes, auto-scaling.<br />- GÃ¨re plusieurs nÅ“uds et applications complexes.<br />- Standard de lâ€™industrie cloud-native. |
| **Machines Virtuelles (VMs)** | - Pour des **environnements fortement isolÃ©s** ou hÃ©tÃ©rogÃ¨nes (Linux, Windows, BSDâ€¦).<br />- Quand la **sÃ©curitÃ©** ou la **compatibilitÃ© OS** est prioritaire. | - Isolation totale (kernel sÃ©parÃ©).- IdÃ©al pour tester plusieurs OS ou infrastructures legacy.<br />- Moins adaptÃ© aux microservices modernes. |

------



# RÃ‰FÃ‰RENCES

[<a id="ref1">1</a>] [https://github.com/Bamolitho/InsideKubernetes](https://github.com/Bamolitho/InsideKubernetes)  

[<a id="ref2">2</a>] [**STRATOSCALE EVERYTHING KUBERNETES: A PRACTICAL GUIDE**](https://iamondemand.com/wp-content/uploads/2019/11/Kubernetes-eBook.pdf)  

**Tech With Nana** : https://youtu.be/-ykwb1d0DXU?si=Mny3zBcnFVE5bXBt